{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import timedelta, datetime\n",
    "import glob\n",
    "from itertools import chain\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "#import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import tensorflow as tf\n",
    "directory = 'D:/ANACONDA/envs/tf-gpu/code/NLP/kakao/data/'\n",
    "font_path = directory + 'NanumGothic.ttf'\n",
    "font_name = fm.FontProperties(fname=font_path, size=10).get_name()\n",
    "plt.rc('font', family=font_name, size=12)\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "['#d6866a498157771069fdf15361cb012b', '#f963fb8c5d9d14d503fc4e80bd8617b4', '#87a6479c91e4276374378f1d28eb307c', '#677e984e245b344f61dc5d3cc1f352c8', '#519f45eb14e4807e8714fb7e835463eb']\n",
      "5000\n",
      "['#7ee14df8642a7925b1465ff5c89efe5b', '#8420b9385b282028eebf1ad6b4a221c0', '#c9b31d8b64357f5854b1ba55b32eb6d3', '#9bb1e13b5481fa3737af20870b25c723', '#37d5f99a7f12c9ba90c4e2ac92e54ab6']\n"
     ]
    }
   ],
   "source": [
    "dev_users_path ='D:/ANACONDA/envs/tf-gpu/code/NLP/kakao/data/predict/predict/dev.users'\n",
    "dev_users_list = []\n",
    "with open(dev_users_path, 'r') as fr:\n",
    "    lines = fr.readlines()\n",
    "    dev_users_list = lines\n",
    "    del lines\n",
    "for i in range(len(dev_users_list)):\n",
    "    dev_users_list[i] = dev_users_list[i].replace('\\n','')\n",
    "    \n",
    "test_users_path ='D:/ANACONDA/envs/tf-gpu/code/NLP/kakao/data/predict/predict/test.users'\n",
    "test_users_list = []\n",
    "with open(test_users_path, 'r') as fr:\n",
    "    lines = fr.readlines()\n",
    "    test_users_list = lines\n",
    "    del lines\n",
    "for i in range(len(test_users_list)):\n",
    "    test_users_list[i] = test_users_list[i].replace('\\n','')\n",
    "\n",
    "print(len(dev_users_list))\n",
    "print(dev_users_list[:5])\n",
    "print(len(test_users_list))\n",
    "print(test_users_list[:5])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Article list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(directory+'article_by_user.json', 'rb') as f:\n",
    "    article_by_nontestuser = json.load(f)\n",
    "with open(directory+'article_by_testuser.json', 'rb') as f:\n",
    "    article_by_testuser = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301222\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(article_by_nontestuser))\n",
    "print(len(article_by_testuser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Contents_textrank \n",
    "with open(directory+'contents_textrank.json', 'rb') as f:\n",
    "    contents_textrank = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article id 로 Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1] make values & corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.391512632369995 sec\n",
      "article_ :  22105708\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "articles = []\n",
    "corpus = []\n",
    "\n",
    "for values in article_by_testuser.values():\n",
    "    for x in values:\n",
    "        articles.append(x)\n",
    "        if(len(values) > 2): # 개수는 더 늘려도 될듯\n",
    "            corpus.append(x)\n",
    "        \n",
    "    if(len(values) != 0):\n",
    "        for i in range(8):\n",
    "            corpus.append('eos')\n",
    "            \n",
    "for values in article_by_nontestuser.values():\n",
    "    for x in values:\n",
    "        articles.append(x)\n",
    "        if(len(values) > 2):\n",
    "            corpus.append(x)\n",
    "    \n",
    "    if(len(values) != 0):\n",
    "        for i in range(8):\n",
    "            corpus.append('eos')\n",
    "        \n",
    "print (\"%s sec\"%(time.time() - start_time))\n",
    "print('article_ : ',len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] Make dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.828672885894775 sec\n"
     ]
    }
   ],
   "source": [
    "# make dictionary\n",
    "# generate dataset, dictionary for word\n",
    "start_time = time.time()\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in articles:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word\n",
    "\n",
    "# 공백 표시할 어휘 : eos\n",
    "word_to_id['eos'] = len(word_to_id)\n",
    "id_to_word[len(word_to_id)-1] = 'eos'\n",
    "# vocab에 없는 단어는 UNK로 표시\n",
    "word_to_id['UNK'] = len(word_to_id)\n",
    "id_to_word[len(word_to_id)-1] = 'UNK'\n",
    "\n",
    "print (\"%s sec\"%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505842\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3] corpus real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- recommend 시 필요없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24461361\n"
     ]
    }
   ],
   "source": [
    "corpus = [word_to_id[w] for w in corpus]\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4] Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- recommend 시 필요없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "import collections\n",
    "def generate_batch(data, batch_size, num_skips, skip_window, data_index):\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "    span = 2 * skip_window + 1                      # context = skip_window + target + skip_window\n",
    "    assert span > num_skips\n",
    "\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)   # 다음 단어 인덱스로 이동. len(data) = 17005207\n",
    "\n",
    "    for i in range(batch_size // num_skips):\n",
    "\n",
    "        targets = list(range(span))     # 1. 0부터 span-1까지의 정수로 채운 다음\n",
    "        targets.pop(skip_window)        # 2. skip_window번째 삭제\n",
    "        np.random.shuffle(targets)      # 3. 난수를 사용해서 섞는다.\n",
    "\n",
    "        start = i * num_skips\n",
    "        batch[start:start+num_skips] = buffer[skip_window]\n",
    "\n",
    "        for j in range(num_skips):\n",
    "            labels[start+j, 0] = buffer[targets[j]]\n",
    "\n",
    "        # 새로운 요소가 들어가면서 가장 먼저 들어간 데이터 삭제\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels, data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: skip-gram 모델 구축\n",
    "import math\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "vocabulary_size =(len(word_to_id))\n",
    "batch_size = 128        # 일반적으로 16 <= batch_size <= 512\n",
    "embedding_size = 128    # embedding vector 크기\n",
    "skip_window = 4         # target 양쪽의 단어 갯수\n",
    "num_skips = 8          # 컨텍스트로부터 생성할 레이블 갯수\n",
    "\n",
    "valid_size = 5     # 유사성을 평가할 단어 집합 크기\n",
    "valid_window = 100  # 앞쪽에 있는 분포들만 뽑기 위한 샘플\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # negative 샘플링 갯수\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "truncated = tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size))\n",
    "nce_weights = tf.Variable(truncated)\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# embeddings 벡터.\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "# 배치 데이터에 대해 NCE loss 평균 계산\n",
    "nce_loss = tf.nn.nce_loss(weights=nce_weights,\n",
    "                          biases=nce_biases,\n",
    "                          labels=train_labels,\n",
    "                          inputs=embed,\n",
    "                          num_sampled=num_sampled,\n",
    "                          num_classes=vocabulary_size)\n",
    "loss = tf.reduce_mean(nce_loss)\n",
    "\n",
    "# SGD optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "# 유사도를 계산하기 위한 모델. 학습 모델은 optimizer까지 구축한 걸로 종료.\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0 : 394.24151611328125\n",
      "Nearest to @yuhaaustin_45: @vitmania86_232, @sarkyuday_19, @rhee1957_85, @jonaspark_48, @cornerstool_354, @prestigegorilla_282, @bongsub-kim_19, @cym8930_77\n",
      "Nearest to @yuhaaustin_11: @myfriendjesus_31, @sunsutu_582, @jeda502_43, @popin-boy_4, @litteratus_27, @4cheol_39, @browne_3, @psychiatricnews_39\n",
      "Nearest to @prestigegorilla_214: @myungsunkim_260, @dreamisme80_330, @quidlesf_48, @doyeonsunim_307, @parksun74_690, @iammystic_29, @hj1003_244, @gangbaram_67\n",
      "Nearest to @yuhaaustin_52: @meether_15, @onestepculture_215, @dbr_58, @hj5098_20, @sweetmeen_7, @oetc21_47, @liha_98, @transpeace_4\n",
      "Nearest to @ladybob_30: @simplestories_379, @a-platform_13, @lovelyming_44, @duk-hyun_37, @kellyluvmore_3, @junatul_96, @topofk_28, @aa3062545_113\n",
      "Average loss at step 10000 : 197.64329766349792\n",
      "Average loss at step 20000 : 124.82415936956406\n",
      "Average loss at step 30000 : 94.77116870567798\n",
      "Average loss at step 40000 : 76.29626282596588\n",
      "Average loss at step 50000 : 64.46992066206933\n",
      "Average loss at step 60000 : 53.18219653202295\n",
      "Average loss at step 70000 : 55.9329263930887\n",
      "Average loss at step 80000 : 41.696005265402796\n",
      "Average loss at step 90000 : 40.1523041903466\n",
      "Average loss at step 100000 : 33.7558518995285\n",
      "Nearest to @yuhaaustin_45: @huruk_4, @needleworm_55, @leeeum_97, @grape_227, @sustainability_29, @tildastudio_41, @chezgonna_55, @bree_81\n",
      "Nearest to @yuhaaustin_11: @elara1020_411, @blossomway_3, @windydog_159, @daily-suuuu_264, @vislakr_457, @carrygrow_42, @wikitree_2674, @yenainparis_1\n",
      "Nearest to @prestigegorilla_214: @moda_15, @1step_72, @hotelscombined_444, @terrydo_75, @leejion_69, @edithstein_22, @anyounggeun_9, @eunzzng0o0_35\n",
      "Nearest to @yuhaaustin_52: @amandaking_52, @lovealice_65, @etcjyp_31, @gang-comic_15, @milaresoltimi_71, @lovly8_50, @kirin1004_242, @whqdmsqkd_536\n",
      "Nearest to @ladybob_30: @apateia98_6, @inner-beauty_113, @goriggi02_70, @etson1_146, @eumryosu_54, @myunghankim_34, @partnerswkakao_100, @rothem_141\n",
      "Average loss at step 110000 : 29.27188763369322\n",
      "Average loss at step 120000 : 28.80260007063523\n",
      "Average loss at step 130000 : 27.633294154973328\n",
      "Average loss at step 140000 : 27.54652573255105\n",
      "Average loss at step 150000 : 21.696073099556564\n",
      "Average loss at step 160000 : 21.142648691493644\n",
      "Average loss at step 170000 : 20.686776982260124\n",
      "Average loss at step 180000 : 19.532022955191135\n",
      "Average loss at step 190000 : 16.572475957284635\n",
      "Average loss at step 200000 : 16.32711503708996\n",
      "Nearest to @yuhaaustin_45: @yuhaaustin_52, @smile-sea_337, @huruk_4, @chezgonna_55, @lamps80_37, @hailey0728_192, @pseudomin_15, @kangsunseng_50\n",
      "Nearest to @yuhaaustin_11: @goodfeel4u_82, @vislakr_457, @lwh0416_17, @elara1020_411, @joongheekim_73, @piux_38, @kangsunseng_118, @yermii_156\n",
      "Nearest to @prestigegorilla_214: @travie_56, @parks9484_6, @csj3814_169, @seriousong_107, @anyounggeun_9, @upside_98, @elang_7, @hermite236_831\n",
      "Nearest to @yuhaaustin_52: @yuhaaustin_45, @137tjdqo_302, @etcjyp_31, @cyranodcd_156, @ksunny-choi_9, @lovly8_50, @wjfehd12_6, @parttimetripper_42\n",
      "Nearest to @ladybob_30: @ladybob_29, @garbar_611, @coologi0812_2, @subface_118, @goriggi02_70, @tnwls9270_2, @itelmen_75, @5345341_35\n",
      "Average loss at step 210000 : 15.774432009730674\n",
      "Average loss at step 220000 : 15.275058703696507\n",
      "Average loss at step 230000 : 13.852014788444805\n",
      "Average loss at step 240000 : 14.783480506440577\n",
      "Average loss at step 250000 : 13.936268375169487\n",
      "Average loss at step 260000 : 13.0157642463075\n",
      "Average loss at step 270000 : 12.757598598418944\n",
      "Average loss at step 280000 : 11.39812406119844\n",
      "Average loss at step 290000 : 11.129012794187268\n",
      "Average loss at step 300000 : 11.534882674845681\n",
      "Nearest to @yuhaaustin_45: @yuhaaustin_52, @smile-sea_337, @yuhaaustin_54, @pwc2214_39, @growingmom_171, @chezgonna_55, @jaehyeonkim_64, @songjh03_57\n",
      "Nearest to @yuhaaustin_11: @yuhaaustin_20, @yuhaaustin_16, @koopost_46, @herreport_182, @wikitree_2674, @yuhaaustin_24, @kwlee3030_10, @yongtmk_29\n",
      "Nearest to @prestigegorilla_214: @seriousong_107, @csj3814_169, @1step_72, @travie_56, @kim432432_29, @edithstein_22, @parks9484_6, @upside_98\n",
      "Nearest to @yuhaaustin_52: @yuhaaustin_45, @greece7_4, @lovly8_50, @gone-gon_19, @dosa1000_168, @paradishift_109, @somd715_19, @rhodeslaw_51\n",
      "Nearest to @ladybob_30: @moment-yet_157, @ladybob_29, @subface_118, @tnwls9270_2, @coologi0812_2, @mariandbook_155, @divewithhana_252, @vomlogue_20\n",
      "Average loss at step 310000 : 12.121437807315402\n",
      "Average loss at step 320000 : 11.560102941222581\n",
      "Average loss at step 330000 : 10.669723996578995\n",
      "Average loss at step 340000 : 10.663037551731989\n",
      "Average loss at step 350000 : 11.36144002056038\n",
      "Average loss at step 360000 : 10.011984938781616\n",
      "Average loss at step 370000 : 9.97158929558806\n",
      "Average loss at step 380000 : 10.710263951727887\n",
      "Average loss at step 390000 : 9.70965018282393\n",
      "Average loss at step 400000 : 8.945197581437696\n",
      "Nearest to @yuhaaustin_45: @yuhaaustin_52, @yuhaaustin_54, @yuhaaustin_10, @songjh03_57, @vitmania86_287, @growingmom_171, @smile-sea_337, @koopost_46\n",
      "Nearest to @yuhaaustin_11: @sequence_32, @koopost_46, @ndr-everyday_236, @yuhaaustin_24, @herreport_182, @wikitree_2674, @kimtk_16, @30story_16\n",
      "Nearest to @prestigegorilla_214: @ilwoncoach_53, @csj3814_169, @ideaaday_5, @edithstein_22, @funder2000_519, @kim432432_29, @seriousong_107, @eumryosu_28\n",
      "Nearest to @yuhaaustin_52: @yuhaaustin_45, @yuhaaustin_54, @yuhaaustin_10, @vegit_140, @leecolove_11, @greece7_4, @biocera_23, @mobiinside_319\n",
      "Nearest to @ladybob_30: @ladybob_29, @ladybob_27, @ladybob_31, @ladybob_24, @ellieyang47uu_191, @sonst7_59, @garbar_611, @itelmen_75\n",
      "Average loss at step 410000 : 10.041046559032425\n",
      "Average loss at step 420000 : 9.979729636436515\n",
      "Average loss at step 430000 : 9.253031037633027\n",
      "Average loss at step 440000 : 8.53871921713082\n",
      "Average loss at step 450000 : 8.388045824132952\n",
      "Average loss at step 460000 : 8.314300158989802\n",
      "Average loss at step 470000 : 8.075302840917464\n",
      "Average loss at step 480000 : 8.066367115796822\n",
      "Average loss at step 490000 : 8.004173276791535\n",
      "Average loss at step 500000 : 7.700104272921663\n",
      "Nearest to @yuhaaustin_45: @yuhaaustin_52, @yuhaaustin_54, @yuhaaustin_10, @yuhaaustin_16, @woody571_61, @smile-sea_337, @hosungko_194, @failit_1\n",
      "Nearest to @yuhaaustin_11: @yuhaaustin_24, @yuhaaustin_16, @yuhaaustin_20, @koopost_46, @yuhaaustin_26, @juliechung96_300, @herreport_182, @sequence_32\n",
      "Nearest to @prestigegorilla_214: @funder2000_519, @ilwoncoach_53, @eumryosu_28, @kim432432_29, @ideaaday_5, @byulpd_393, @floralboat_182, @dudndudndudn_44\n",
      "Nearest to @yuhaaustin_52: @yuhaaustin_45, @yuhaaustin_54, @yuhaaustin_10, @greece7_4, @cornerstool_380, @doooboo_52, @gone-gon_19, @futureshaper_24\n",
      "Nearest to @ladybob_30: @ladybob_29, @ladybob_27, @itelmen_75, @jo2660_56, @ladybob_31, @moonyment_27, @dys04090_58, @ryuj_110\n",
      "Average loss at step 510000 : 7.623491498467885\n",
      "Average loss at step 520000 : 7.67691591739133\n",
      "Average loss at step 530000 : 8.179820392404613\n",
      "Average loss at step 540000 : 7.409150019359123\n",
      "Average loss at step 550000 : 7.446145219605416\n",
      "Average loss at step 560000 : 8.185864974877331\n",
      "Average loss at step 570000 : 7.583549774497375\n",
      "Average loss at step 580000 : 7.239635247616842\n",
      "Average loss at step 590000 : 7.365619937143615\n",
      "Average loss at step 600000 : 7.092536019258108\n",
      "Nearest to @yuhaaustin_45: @yuhaaustin_52, @yuhaaustin_54, @yuhaaustin_10, @dusghk1001_11, @speralist_229, @khgina_13, @soosuhada_5, @seunghoon82_28\n",
      "Nearest to @yuhaaustin_11: @yuhaaustin_24, @yuhaaustin_16, @yuhaaustin_26, @yuhaaustin_10, @30story_16, @herreport_182, @yuhaaustin_20, @koopost_46\n",
      "Nearest to @prestigegorilla_214: @kim432432_29, @funder2000_519, @ilwoncoach_53, @floralboat_182, @eumryosu_28, @csj3814_169, @doyeonsunim_192, @byulpd_393\n",
      "Nearest to @yuhaaustin_52: @yuhaaustin_45, @dusghk1001_11, @futureshaper_24, @yuhaaustin_54, @speralist_229, @writing-ay_177, @doooboo_52, #06f55fd46b1e1efdf1e82c621e974630_4\n",
      "Nearest to @ladybob_30: @ladybob_29, @ladybob_27, @ladybob_31, @itelmen_75, @summ-er_54, @jo2660_56, @moment-yet_157, @lunarshore_330\n",
      "Average loss at step 610000 : 7.324022748157149\n",
      "Average loss at step 620000 : 7.397497332708631\n",
      "Average loss at step 630000 : 7.2168260183131325\n",
      "Average loss at step 640000 : 7.271205308157206\n",
      "Average loss at step 650000 : 7.067075082467776\n",
      "Average loss at step 660000 : 6.740531535678613\n",
      "Average loss at step 670000 : 6.569866879352928\n",
      "Average loss at step 680000 : 6.815034143824317\n",
      "Average loss at step 690000 : 6.741862552472821\n",
      "Average loss at step 700000 : 10.484563326384873\n",
      "Nearest to @yuhaaustin_45: @yuhaaustin_52, @yuhaaustin_54, @yuhaaustin_10, @hosungko_194, @yuhaaustin_16, @speralist_229, @dusghk1001_11, @soosuhada_5\n",
      "Nearest to @yuhaaustin_11: @yuhaaustin_24, @yuhaaustin_16, @yuhaaustin_10, @yuhaaustin_26, @yuhaaustin_20, @30story_16, @yuhaaustin_45, @park-so-hyun_34\n",
      "Nearest to @prestigegorilla_214: @funder2000_519, @eumryosu_28, @hotelscombined_444, @ilwoncoach_53, @byulpd_393, @seriousong_107, @kim432432_29, @floralboat_182\n",
      "Nearest to @yuhaaustin_52: @yuhaaustin_45, @yuhaaustin_54, @yuhaaustin_10, @yuhaaustin_24, @dusghk1001_11, @speralist_229, @canghl24_177, @lawrence435_127\n",
      "Nearest to @ladybob_30: @ladybob_29, @ladybob_27, @ladybob_25, @ladybob_26, @itelmen_75, @tingyan_74, @moonjakga_248, @namgizaa_46\n",
      "Average loss at step 710000 : 6.955285946936346\n",
      "Average loss at step 720000 : 6.384752627649903\n",
      "Average loss at step 730000 : 6.483326333491597\n",
      "Average loss at step 740000 : 7.368167270538735\n",
      "Average loss at step 750000 : 6.426533931057714\n",
      "Average loss at step 760000 : 7.031955075389054\n",
      "Average loss at step 770000 : 6.690861252844986\n",
      "Average loss at step 780000 : 6.554198711076844\n",
      "Average loss at step 790000 : 6.512160350650921\n",
      "Average loss at step 800000 : 6.2851709366483615\n",
      "Nearest to @yuhaaustin_45: @yuhaaustin_52, @yuhaaustin_54, @yuhaaustin_10, @brunchflgu_114, @nergul01_157, @speralist_229, @hosungko_194, @cornerstool_380\n",
      "Nearest to @yuhaaustin_11: @yuhaaustin_24, @yuhaaustin_16, @yuhaaustin_26, @yuhaaustin_20, @yuhaaustin_10, @30story_16, @supersteak_2, @seonkicheong_644\n",
      "Nearest to @prestigegorilla_214: @jiney_107, @ideaaday_5, @myolivenote_831, @funder2000_519, @eumryosu_28, @hotelscombined_444, @byulpd_393, @floralboat_182\n",
      "Nearest to @yuhaaustin_52: @yuhaaustin_45, @yuhaaustin_54, @yuhaaustin_10, @gone-gon_19, @lawrence435_127, @doubleshot_516, @dusghk1001_11, @poldee_18\n",
      "Nearest to @ladybob_30: @prestigegorilla_346, @ladybob_29, @haereka_14, @namgizaa_46, @ladybob_31, @kidjaydiary_20, @ladybob_25, @arob_2\n",
      "Average loss at step 810000 : 6.0708397787041966\n",
      "Average loss at step 820000 : 6.433150347458292\n",
      "Average loss at step 830000 : 6.171626444831304\n",
      "Average loss at step 840000 : 6.193343000166397\n",
      "Average loss at step 850000 : 6.308307156286482\n",
      "Average loss at step 860000 : 6.276911036809348\n",
      "Average loss at step 870000 : 5.954867113080062\n",
      "Average loss at step 880000 : 6.01421575002335\n",
      "Average loss at step 890000 : 6.560440279283934\n",
      "Average loss at step 900000 : 6.3601051881665365\n",
      "Nearest to @yuhaaustin_45: @yuhaaustin_52, @yuhaaustin_54, @sooband_115, @greece7_4, @yuhaaustin_20, @yearning_55, @josephlee54_218, @oemilk_17\n",
      "Nearest to @yuhaaustin_11: @yuhaaustin_16, @yuhaaustin_24, @yuhaaustin_10, @gunbbang_621, @yuhaaustin_20, @dailylife_666, @yuhaaustin_26, @janelee93_56\n",
      "Nearest to @prestigegorilla_214: @daljasee_208, @jiney_107, @ideaaday_5, @yhiyhiyhiyhi_1, @funder2000_519, @honeytip_1115, @dailylife_444, @eumryosu_28\n",
      "Nearest to @yuhaaustin_52: @yuhaaustin_45, @yuhaaustin_54, @miu_284, @sprout_2, @truth-art_63, @poldee_18, @gone-gon_19, @beanprofiler_137\n",
      "Nearest to @ladybob_30: @ladybob_29, @kidjaydiary_6, @ladybob_31, @lazymoooon_3, @brusohee_16, @purmae33_104, @minsungdkim_234, @prestigegorilla_346\n",
      "Average loss at step 910000 : 5.85928580601057\n",
      "Average loss at step 920000 : 6.61835523041999\n",
      "Average loss at step 930000 : 6.205910850568302\n",
      "Average loss at step 940000 : 5.844774160124454\n",
      "Average loss at step 950000 : 5.5078940402047705\n",
      "Average loss at step 960000 : 5.543145314289443\n",
      "Average loss at step 970000 : 6.663501517590229\n",
      "Average loss at step 980000 : 5.8273727964558635\n",
      "Average loss at step 990000 : 5.6401941809485665\n",
      "Average loss at step 1000000 : 5.598703094003163\n",
      "Nearest to @yuhaaustin_45: @yuhaaustin_52, @yuhaaustin_54, @yuhaaustin_20, @yuhaaustin_10, @thepazzikang_2, @greece7_4, @yuhaaustin_24, @iamhere_24\n",
      "Nearest to @yuhaaustin_11: @yuhaaustin_16, @yuhaaustin_24, @yuhaaustin_20, @yuhaaustin_10, @yuhaaustin_26, @angiesongc9sx_37, @seonkicheong_644, @park-so-hyun_34\n",
      "Nearest to @prestigegorilla_214: @dailylife_444, @jiney_107, @daljasee_208, @yhiyhiyhiyhi_1, @ideaaday_5, @funder2000_519, @everythingisgag_197, @honeytip_1115\n",
      "Nearest to @yuhaaustin_52: @yuhaaustin_45, @yuhaaustin_54, @yuhaaustin_10, @yuhaaustin_20, @yuhaaustin_16, @poldee_18, @wm--ys_89, @sprout_2\n",
      "Nearest to @ladybob_30: @ladybob_31, @ladybob_29, @peachmember_24, @prestigegorilla_346, @minjilunajung_73, @ladybob_24, @gardrna0817_67, @ohjanelee_6\n"
     ]
    }
   ],
   "source": [
    "# Step 5: skip-gram 모델 학습\n",
    "# 4시 4분 시작\n",
    "num_steps = 1000001\n",
    "data = corpus\n",
    "ordered_words = list(word_to_id.keys())\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    average_loss, data_index = 0, 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels, data_index = generate_batch(data, batch_size, num_skips, skip_window, data_index)\n",
    "\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        # 마지막 10000번에 대한 평균 loss 표시\n",
    "        if step % 10000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 10000\n",
    "            print('Average loss at step {} : {}'.format(step, average_loss))\n",
    "            average_loss = 0\n",
    "        \n",
    "        # 10만번째마다 valid size만큼 sim 계산\n",
    "        if step % 100000 == 0:\n",
    "            sim = similarity.eval()         # (16, vocab_size)\n",
    "\n",
    "            for i in range(valid_size):\n",
    "                valid_word = ordered_words[valid_examples[i]]\n",
    "\n",
    "                top_k = 8\n",
    "                nearest = sim[i].argsort()[-top_k - 1:-1][::-1]\n",
    "                log_str = ', '.join([ordered_words[k] for k in nearest])\n",
    "                print('Nearest to {}: {}'.format(valid_word, log_str))\n",
    "\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(session,'keyword_embedding_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(directory+'article_embedding_matrix',final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_json(directory + 'metadata.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_embeddings_matrix =  np.load(directory+'article_embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(directory+'article_embedding_matrix_1', 'wb') as f:\n",
    "    pickle.dump(article_embeddings_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(505842, 128)\n",
      "<class 'numpy.ndarray'>\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(article_embeddings_matrix.shape)\n",
    "print(type(article_embeddings_matrix))\n",
    "print(article_embeddings_matrix.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505842"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_data = list(word_to_id.keys())\n",
    "len(article_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "embeddings_article = tf.get_variable(\"artcle_matrix\",initializer = article_embeddings_matrix)\n",
    "norm_article = tf.sqrt(tf.reduce_sum(tf.square(embeddings_article),1,keep_dims =True))\n",
    "normalized_embeddings_article = embeddings_article / norm_article\n",
    "\n",
    "embeddings_article_vector = tf.get_variable(\"article_vector\",initializer = article_embeddings_matrix[0]) ## will be transposed\n",
    "embeddings_article_vector = tf.reshape(embeddings_article_vector,(128,1),)\n",
    "norm_article_t = tf.sqrt(tf.reduce_sum(tf.square(embeddings_article_vector),1,keep_dims =True))\n",
    "normalized_embeddings_article_t = embeddings_article_vector / norm_article_t\n",
    "\n",
    "\n",
    "similarity = tf.matmul(normalized_embeddings_article, normalized_embeddings_article_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "sim_article = tf.placeholder(tf.int32, shape=[len(article_data),top_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(x,y,eps=1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x**2))+eps)\n",
    "    ny = y / (np.sqrt(np.sum(y**2))+eps)\n",
    "    return np.dot(nx,ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.87090516090393 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "cos_sim = []\n",
    "for i in range(len(article_data)):\n",
    "    cos_sim.append(cos_similarity(article_embeddings_matrix[i],article_embeddings_matrix[0]))\n",
    "    cos_s = np.asarray(cos_sim)\n",
    "    cos_sim = cos_s.argsort()[-top_k-1:-1][::-1].tolist()\n",
    "print (\"%s sec\"%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4791829586029053 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "cos_sim = []\n",
    "for i in range(50000):\n",
    "    cos_sim.append(cos_similarity(article_embeddings_matrix[i],article_embeddings_matrix[0]))\n",
    "    cos_s = np.asarray(cos_sim)\n",
    "    cos_sim = cos_s.argsort()[-top_k-1:-1][::-1].tolist()\n",
    "print (\"%s sec\"%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.33065676689148 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sim = similarity.eval()\n",
    "    sim_top_10 = sim.argsort()[-top_k-1:-1][::-1].tolist()\n",
    "print (\"%s sec\"%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_embeddings_matrix[0].reshape(128,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_loop_spatial(matrix, vector):\n",
    "    \"\"\"\n",
    "    Calculating pairwise cosine distance using a common for loop with the numpy cosine function.\n",
    "    \"\"\"\n",
    "    neighbors = []\n",
    "    for row in range(matrix.shape[0]):\n",
    "        neighbors.append(scipy.spatial.distance.cosine(vector, matrix[row,:]))\n",
    "    return neighbors\n",
    "def cos_loop(matrix, vector):\n",
    "    \"\"\"\n",
    "    Calculating pairwise cosine distance using a common for loop with manually calculated cosine value.\n",
    "    \"\"\"\n",
    "    neighbors = []\n",
    "    for row in range(matrix.shape[0]):\n",
    "        vector_norm = np.linalg.norm(vector)\n",
    "        row_norm = np.linalg.norm(matrix[row,:])\n",
    "        cos_val = vector.dot(matrix[row,:]) / (vector_norm * row_norm)\n",
    "        neighbors.append(cos_val)\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_matrix_multiplication(matrix, vector):\n",
    "    \"\"\"\n",
    "    Calculating pairwise cosine distance using matrix vector multiplication.\n",
    "    \"\"\"\n",
    "    dotted = matrix.dot(vector)\n",
    "    matrix_norms = np.linalg.norm(matrix, axis=1)\n",
    "    vector_norm = np.linalg.norm(vector)\n",
    "    matrix_vector_norms = np.multiply(matrix_norms, vector_norm)\n",
    "    neighbors = np.divide(dotted, matrix_vector_norms)\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16212916374206543 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "cos_matrix_multiplication(article_embeddings_matrix, article_embeddings_matrix[0])\n",
    "\n",
    "print (\"%s sec\"%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(directory+'gensim.txt' ,'w', encoding='utf8')\n",
    "f.write(f'{len(word_to_id)} {128}\\n')\n",
    "\n",
    "word_vecs = article_embeddings_matrix  # W Matrix\n",
    "for word, idx in word_to_id.items():\n",
    "    str_vec = ' '.join(map(str, list(word_vecs[idx, :])))\n",
    "    f.write(f'{word} {str_vec}\\n')\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_cbow = KeyedVectors.load_word2vec_format(directory+'gensim.txt', unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02201700210571289\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "w2v_cbow.most_similar(id_to_word[0])\n",
    "print(time.time() -s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-e51713d2fb09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mw2v_cbow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'gensim.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mw2v_cbow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_to_word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mline_no\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "w2v_cbow = KeyedVectors.load_word2vec_format(directory+'gensim.txt', unicode_errors='ignore')\n",
    "list(tokenizer.word_index.keys())[:10]\n",
    "w2v_cbow.most_similar(id_to_word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                           | 85/505842 [00:17<29:15:57,  4.80it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-f50fddd439d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcos_sim_list\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmininterval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mcos_sim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcos_matrix_multiplication\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_embeddings_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marticle_embeddings_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mcos_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcos_sim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mcos_sim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcos_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-e4d057de3765>\u001b[0m in \u001b[0;36mcos_matrix_multiplication\u001b[1;34m(matrix, vector)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m      5\u001b[0m     \u001b[0mdotted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmatrix_norms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mvector_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmatrix_vector_norms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix_norms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2478\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mord\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mord\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2479\u001b[0m             \u001b[1;31m# special case for speedup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2480\u001b[1;33m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2481\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2482\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                           | 85/505842 [00:30<29:15:57,  4.80it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "top_k = 10\n",
    "cos_sim_list =[]\n",
    "for i in tqdm(range(len(word_to_id)),mininterval=1):\n",
    "    cos_sim = cos_matrix_multiplication(article_embeddings_matrix, article_embeddings_matrix[i])\n",
    "    cos_s = np.asarray(cos_sim)\n",
    "    cos_sim = cos_s.argsort()[-top_k-1:-1][::-1].tolist()\n",
    "    cos_sim_list.append(cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# matrix, vecottor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.92854332923889 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import scipy.spatial.distance\n",
    "result = cos_loop_spatial(article_embeddings_matrix, article_embeddings_matrix[0])\n",
    "for i in range(len(result)):\n",
    "    result[i] = 1 - result[i]\n",
    "print (\"%s sec\"%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_top_10 = result.argsort()[-top_k -1 : -1][::-1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_top_10_article = [ id_to_word[x] for x in result_top_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_embeddings_matrix[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-3424ba6ae42b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_embeddings_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0marticle_embeddings_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msquare_mag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0minv_square_mag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msquare_mag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0minv_square_mag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minv_square_mag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\numpy\\lib\\twodim_base.py\u001b[0m in \u001b[0;36mdiag\u001b[1;34m(v, k)\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "similarity = np.dot(article_embeddings_matrix,article_embeddings_matrix[0])\n",
    "square_mag = np.diag(similarity)\n",
    "inv_square_mag = 1 / square_mag\n",
    "inv_square_mag[np.isinf(inv_square_mag)] = 0\n",
    "inv_mag = np.sqrt(inv_square_mag)\n",
    "cosine = similarity * inv_mag\n",
    "cosine = cosine.T * inv_mag\n",
    "\n",
    "print (\"%s sec\"%(time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_keyword = []\n",
    "start_time = time.time()\n",
    "for value in contents_textrank.values():\n",
    "    for x in value:\n",
    "        contents_keyword.append(x)\n",
    "\n",
    "print (\"%s sec\"%(time.time() - start_time))\n",
    "print('text_keyword : ',len(contents_keyword))\n",
    "\n",
    "# make dictionary\n",
    "# generate dataset, dictionary for word\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in contents_keyword:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word\n",
    "# 공백 표시할 어휘 : eos\n",
    "word_to_id['eos'] = len(word_to_id)\n",
    "id_to_word[len(word_to_id)-1] = 'eos'\n",
    "# vocab에 없는 단어는 UNK로 표시\n",
    "word_to_id['UNK'] = len(word_to_id)\n",
    "id_to_word[len(word_to_id)-1] = 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [23:50<00:00,  3.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# Too slow..\n",
    "article_by_user = {}\n",
    "\n",
    "# Using Multiprocessing\n",
    "from multiprocessing import Pool\n",
    "article_by_user = {}\n",
    "for i in tqdm(range(len(test_users_list))):\n",
    "    article_by_user[i] = extract_article_by_user(test_users_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(article_by_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(directory+'article_by_testuser.json', 'w', encoding=\"utf-8\") as make_file:\n",
    "    json.dump(article_by_user, make_file, ensure_ascii=False, indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.142323017120361\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "def count(name):\n",
    "    for i in range(1,50000000):\n",
    "        l = 1\n",
    "    return 1\n",
    "num_list = ['p1','p2','p3','p4']\n",
    "for num in num_list:\n",
    "    count(num)\n",
    "    \n",
    "print (\"%s\"%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'defs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-fba875de5fc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdefs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'defs'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import defs\n",
    "start_time = time.time()\n",
    "def count(name):\n",
    "    for i in range(1,50000000):\n",
    "        l = 1\n",
    "num_list = ['p1','p2','p3','p4']\n",
    "if __name__ == '__main__':\n",
    "    pool = Pool()\n",
    "    results = pool.map(defs.count,numlist)\n",
    "\n",
    "    \n",
    "print (\"%s\"%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
