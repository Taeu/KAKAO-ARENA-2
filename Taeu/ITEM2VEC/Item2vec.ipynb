{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import timedelta, datetime\n",
    "import glob\n",
    "from itertools import chain\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "#import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import tensorflow as tf\n",
    "directory = 'D:/ANACONDA/envs/tf-gpu/code/NLP/kakao/data/'\n",
    "font_path = directory + 'NanumGothic.ttf'\n",
    "font_name = fm.FontProperties(fname=font_path, size=10).get_name()\n",
    "plt.rc('font', family=font_name, size=12)\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "['#d6866a498157771069fdf15361cb012b', '#f963fb8c5d9d14d503fc4e80bd8617b4', '#87a6479c91e4276374378f1d28eb307c', '#677e984e245b344f61dc5d3cc1f352c8', '#519f45eb14e4807e8714fb7e835463eb']\n",
      "5000\n",
      "['#7ee14df8642a7925b1465ff5c89efe5b', '#8420b9385b282028eebf1ad6b4a221c0', '#c9b31d8b64357f5854b1ba55b32eb6d3', '#9bb1e13b5481fa3737af20870b25c723', '#37d5f99a7f12c9ba90c4e2ac92e54ab6']\n"
     ]
    }
   ],
   "source": [
    "dev_users_path ='D:/ANACONDA/envs/tf-gpu/code/NLP/kakao/data/predict/predict/dev.users'\n",
    "dev_users_list = []\n",
    "with open(dev_users_path, 'r') as fr:\n",
    "    lines = fr.readlines()\n",
    "    dev_users_list = lines\n",
    "    del lines\n",
    "for i in range(len(dev_users_list)):\n",
    "    dev_users_list[i] = dev_users_list[i].replace('\\n','')\n",
    "    \n",
    "test_users_path ='D:/ANACONDA/envs/tf-gpu/code/NLP/kakao/data/predict/predict/test.users'\n",
    "test_users_list = []\n",
    "with open(test_users_path, 'r') as fr:\n",
    "    lines = fr.readlines()\n",
    "    test_users_list = lines\n",
    "    del lines\n",
    "for i in range(len(test_users_list)):\n",
    "    test_users_list[i] = test_users_list[i].replace('\\n','')\n",
    "\n",
    "print(len(dev_users_list))\n",
    "print(dev_users_list[:5])\n",
    "print(len(test_users_list))\n",
    "print(test_users_list[:5])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Article list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(directory+'article_by_user.json', 'rb') as f:\n",
    "    article_by_nontestuser = json.load(f)\n",
    "\n",
    "with open(directory+'article_by_testuser.json', 'rb') as f:\n",
    "    article_by_testuser = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301222\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(article_by_nontestuser))\n",
    "print(len(article_by_testuser))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## article list processing \n",
    "- 최신 글부터 중복된 글을 제거하고 다시 dictionary를 만듦 (최신 글이 중요하다는 생각으로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.53485441207886 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "article_by_nontestuser_t = dict()\n",
    "for key,value in article_by_nontestuser.items() :\n",
    "    key = int(key)\n",
    "    s = []\n",
    "    value = list(reversed(value))\n",
    "    for i in range(len(value)):\n",
    "        if value[i] not in s:\n",
    "            s.append(value[i])\n",
    "    s = list(reversed(s))\n",
    "    article_by_nontestuser_t[key] = s\n",
    "print(time.time() - start_time , 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301222\n"
     ]
    }
   ],
   "source": [
    "print(len(article_by_nontestuser_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.241394519805908 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "article_by_testuser_t = dict()\n",
    "for key,value in article_by_testuser.items() :\n",
    "    key = int(key)\n",
    "    s = []\n",
    "    value = list(reversed(value))\n",
    "    for i in range(len(value)):\n",
    "        if value[i] not in s:\n",
    "            s.append(value[i])\n",
    "    s = list(reversed(s))\n",
    "    article_by_testuser_t[key] = s\n",
    "    \n",
    "print(time.time() - start_time , 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(article_by_testuser_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article 중복 제거 해보기 (역순으로)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Procesing Again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test_users 가 읽은 글들을 다시 뽑기 (5000개의 글) - 제대로 됐음\n",
    "\n",
    "- article_by_nontestuser_t\n",
    "- article_by_testuser_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] Make Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.146909952163696 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "read_file_lst = glob.glob(directory + 'read/read/*')\n",
    "exclude_file_lst = ['read.tar']\n",
    "read_df_lst = []\n",
    "for f in read_file_lst:\n",
    "    file_name = os.path.basename(f)\n",
    "    if file_name in exclude_file_lst:\n",
    "        print(file_name)\n",
    "    else:\n",
    "        df_temp = pd.read_csv(f, header=None, names=['raw'])\n",
    "        df_temp['dt'] = file_name[:8]\n",
    "        df_temp['hr'] = file_name[8:10]\n",
    "        df_temp['user_id'] = df_temp['raw'].str.split(' ').str[0]\n",
    "        df_temp['article_id'] = df_temp['raw'].str.split(' ').str[1:].str.join(' ').str.strip()\n",
    "        read_df_lst.append(df_temp)\n",
    "        \n",
    "read = pd.concat(read_df_lst)\n",
    "print (\"%s sec\"%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22110706, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chainer(s):\n",
    "    return list(chain.from_iterable(s.str.split(' ')))\n",
    "read_cnt_by_user = read['article_id'].str.split(' ').map(len)\n",
    "read_raw = pd.DataFrame({'dt': np.repeat(read['dt'], read_cnt_by_user),\n",
    "                         'hr': np.repeat(read['hr'], read_cnt_by_user),\n",
    "                         'user_id': np.repeat(read['user_id'], read_cnt_by_user),\n",
    "                         'article_id': chainer(read['article_id'])})\n",
    "read_raw.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_json(directory + 'metadata.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "atc = metadata.copy()\n",
    "atc['reg_datetime'] = atc['reg_ts'].apply(lambda x : datetime.fromtimestamp(x/1000.0))\n",
    "atc.loc[atc['reg_datetime'] == atc['reg_datetime'].min(), 'reg_datetime'] = datetime(2090, 12, 31)\n",
    "atc['reg_dt'] = atc['reg_datetime'].dt.date\n",
    "atc['type'] = atc['magazine_id'].apply(lambda x : '개인' if x == 0.0 else '매거진')\n",
    "# 컬럼명 변경\n",
    "atc.columns = ['id', 'display_url', 'article_id', 'keyword_list', 'magazine_id', 'reg_ts', 'sub_title', 'title', 'author_id', 'reg_datetime', 'reg_dt', 'type']\n",
    "atc.head()\n",
    "atc_cnt_by_reg_dt = atc.groupby('reg_dt', as_index=False)['article_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "atc_read_cnt = read_raw[read_raw.article_id != ''].groupby('article_id')['user_id'].count()\n",
    "atc_read_cnt = atc_read_cnt.reset_index()\n",
    "atc_read_cnt.columns = ['article_id', 'read_cnt']\n",
    "#metadata 결합\n",
    "atc_read_cnt = pd.merge(atc_read_cnt, atc, how='left', left_on='article_id', right_on='article_id')\n",
    "# metadata를 찾을 수 없는 소비 로그 제외\n",
    "atc_read_cnt_nn = atc_read_cnt[atc_read_cnt['id'].notnull()]\n",
    "# 소비수 기준 분류값\n",
    "#atc_read_cnt_nn.sort_values(by='read_cnt', ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### len(20) 미만 버리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134991\n",
      "0.14211320877075195 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "atc_read_cnt = atc_read_cnt_nn[atc_read_cnt_nn['read_cnt'] > 20]\n",
    "article_vocab_list = atc_read_cnt['article_id'].tolist()\n",
    "print(len(article_vocab_list))\n",
    "print (\"%s sec\"%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341156\n",
      "0.14711737632751465 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "atc_read_cnt_less = atc_read_cnt_nn[atc_read_cnt_nn['read_cnt'] <= 20]\n",
    "article_vocab_list_less = atc_read_cnt_less['article_id'].tolist()\n",
    "print(len(article_vocab_list_less))\n",
    "print (\"%s sec\"%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- article_by_nontestuser_t\n",
    "- article_by_testuser_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_vocab_list_nontestuser_t = set()\n",
    "for values in article_by_nontestuser_t.values():\n",
    "    for x in values:\n",
    "        article_vocab_list_nontestuser_t.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_vocab_list_testuser_t = set()\n",
    "for values in article_by_testuser_t.values():\n",
    "    for x in values:\n",
    "        article_vocab_list_testuser_t.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_vocab_list_byuser = article_vocab_list_nontestuser_t | article_vocab_list_testuser_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134991\n",
      "505840\n",
      "341156\n"
     ]
    }
   ],
   "source": [
    "print(len(article_vocab_list))\n",
    "print(len(article_vocab_list_byuser))\n",
    "print(len(article_vocab_list_less))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_vocab_list_byuser = list(set(article_vocab_list_byuser) - set(article_vocab_list))\n",
    "article_vocab_list_less = list(set(article_vocab_list_less) - set(article_vocab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_vocab_list_byuser = list(set(article_vocab_list_byuser) - set(article_vocab_list_less))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134991\n",
      "29693\n",
      "341156\n"
     ]
    }
   ],
   "source": [
    "print(len(article_vocab_list))\n",
    "print(len(article_vocab_list_byuser))\n",
    "print(len(article_vocab_list_less))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370849\n"
     ]
    }
   ],
   "source": [
    "nonpopular_article_vocab_list = article_vocab_list_less + article_vocab_list_byuser\n",
    "print(len(nonpopular_article_vocab_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dict 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09408259391784668 sec\n"
     ]
    }
   ],
   "source": [
    "# make dictionary\n",
    "# generate dataset, dictionary for word\n",
    "start_time = time.time()\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in article_vocab_list:\n",
    "    new_id = len(word_to_id)\n",
    "    word_to_id[word] = new_id\n",
    "    id_to_word[new_id] = word\n",
    "\n",
    "# 공백 표시할 어휘 : eos\n",
    "word_to_id['eos'] = len(word_to_id)\n",
    "id_to_word[len(word_to_id)-1] = 'eos'\n",
    "# vocab에 없는 단어는 UNK로 표시\n",
    "word_to_id['UNK'] = len(word_to_id)\n",
    "id_to_word[len(word_to_id)-1] = 'UNK'\n",
    "\n",
    "print (\"%s sec\"%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134993\n",
      "134993\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_id))\n",
    "print(len(id_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_id = len(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134993"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in nonpopular_article_vocab_list:\n",
    "    word_to_id[word] = fix_id\n",
    "    id_to_word[fix_id] = 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505842\n",
      "134994\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_id))\n",
    "print(len(id_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(directory + 'word_to_id.json','w') as f:\n",
    "    json.dump(word_to_id,f)\n",
    "    \n",
    "with open(directory + 'id_to_word.json','w') as f:\n",
    "    json.dump(id_to_word,f)\n",
    "\n",
    "with open(directory + 'word_to_id.json') as f:\n",
    "    word_to_id  = json.load(f)\n",
    "\n",
    "with open(directory + 'id_to_word.json') as f:\n",
    "    id_to_word = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] make corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_article_list_by_users = list(article_by_nontestuser_t.values()) + list(article_by_testuser_t.values())\n",
    "testuser_article_list_by_users = list(article_by_testuser_t.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(total_article_list_by_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8805170059204102 sec\n",
      "13907947\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "corpus = []\n",
    "\n",
    "  \n",
    "for values in total_article_list_by_users:\n",
    "\n",
    "    if(len(values) > 4):\n",
    "        for x in values:\n",
    "            corpus.append(x)\n",
    "    \n",
    "    if(len(values) > 4):\n",
    "        for i in range(8):\n",
    "            corpus.append('eos')\n",
    "        \n",
    "        \n",
    "print (\"%s sec\"%(time.time() - start_time))\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.974085330963135 sec\n",
      "13907947\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "corpus_r = []\n",
    "corpus_r = [word_to_id[w] for w in corpus]\n",
    "print (\"%s sec\"%(time.time() - start_time))\n",
    "print(len(corpus_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4] Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- recommend 시 필요없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "import collections\n",
    "def generate_batch(data, batch_size, num_skips, skip_window, data_index):\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "    span = 2 * skip_window + 1                      # context = skip_window + target + skip_window\n",
    "    assert span > num_skips\n",
    "\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)   # 다음 단어 인덱스로 이동. len(data) = 17005207\n",
    "\n",
    "    for i in range(batch_size // num_skips):\n",
    "\n",
    "        targets = list(range(span))     # 1. 0부터 span-1까지의 정수로 채운 다음\n",
    "        targets.pop(skip_window)        # 2. skip_window번째 삭제\n",
    "        np.random.shuffle(targets)      # 3. 난수를 사용해서 섞는다.\n",
    "\n",
    "        start = i * num_skips\n",
    "        batch[start:start+num_skips] = buffer[skip_window]\n",
    "\n",
    "        for j in range(num_skips):\n",
    "            labels[start+j, 0] = buffer[targets[j]]\n",
    "\n",
    "        # 새로운 요소가 들어가면서 가장 먼저 들어간 데이터 삭제\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels, data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0721 17:03:08.941899 22088 deprecation.py:323] From D:\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0721 17:03:09.069992 22088 deprecation.py:506] From <ipython-input-39-2fea1435b5f3>:42: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# Step 4: skip-gram 모델 구축\n",
    "import math\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "vocabulary_size =(len(id_to_word)) ################################################# 수정해주기!!\n",
    "batch_size = 128        # 일반적으로 16 <= batch_size <= 512\n",
    "embedding_size = 128    # embedding vector 크기\n",
    "skip_window = 4         # target 양쪽의 단어 갯수\n",
    "num_skips = 8          # 컨텍스트로부터 생성할 레이블 갯수\n",
    "\n",
    "valid_size = 5     # 유사성을 평가할 단어 집합 크기\n",
    "valid_window = 15000  # 앞쪽에 있는 분포들만 뽑기 위한 샘플\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # negative 샘플링 갯수\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "truncated = tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size))\n",
    "nce_weights = tf.Variable(truncated)\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# embeddings 벡터.\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "# 배치 데이터에 대해 NCE loss 평균 계산\n",
    "nce_loss = tf.nn.nce_loss(weights=nce_weights,\n",
    "                          biases=nce_biases,\n",
    "                          labels=train_labels,\n",
    "                          inputs=embed,\n",
    "                          num_sampled=num_sampled,\n",
    "                          num_classes=vocabulary_size)\n",
    "loss = tf.reduce_mean(nce_loss)\n",
    "\n",
    "# SGD optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "# 유사도를 계산하기 위한 모델. 학습 모델은 optimizer까지 구축한 걸로 종료.\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0 : 334.28741455078125\n",
      "Nearest to @artinsight_399: @jaol_1, @shoong810_192, @jumjan_257, @plusx_26, @vpluscharles_42, @kecologist_70, @hopeingyu_5, @kyleso_7\n",
      "Nearest to @bestar_31: @barneconomy_79, @readingboy_43, @jawssss_13, @videsign20_16, @likewater_23, @brunch5nga_3, @munchi_65, @babydiary_146\n",
      "Nearest to @alexkang_1012: @iamyoungzoo_17, @myungsunkim_269, @syn701_75, @leeojsh_22, @ljg523_1753, @tenbody_1649, @sustainability_31, @lookfar_27\n",
      "Nearest to @altctrlshift_35: @lookfar_164, @20timeline_33, @brunchqxk5_181, @allstay_1044, @yunj814_94, @leedh200_10, @ujinnew_7, @brunch1uhl_567\n",
      "Nearest to @13july_65: @needleworm_10, @gians_13, @everj76_161, @mokiusagi_12, @failit_28, @znryu_38, @beaurin58_5, @malcom_46\n",
      "Average loss at step 10000 : 113.89247313814163\n",
      "Average loss at step 20000 : 46.4074169931364\n",
      "Average loss at step 30000 : 29.108286816103348\n",
      "Average loss at step 40000 : 19.39349226460999\n",
      "Average loss at step 50000 : 14.566657371038467\n",
      "Average loss at step 60000 : 11.982359294262576\n",
      "Average loss at step 70000 : 9.498762971335696\n",
      "Average loss at step 80000 : 8.976881295672863\n",
      "Average loss at step 90000 : 8.051787123952495\n",
      "Average loss at step 100000 : 8.790183675263473\n",
      "Nearest to @artinsight_399: @dungdang_30, @slist_1326, @windydog_12, @tnwlsl0810_16, @leesigi_37, @sunami1617_17, @whqdmsqkd_614, @isarerus_181\n",
      "Nearest to @bestar_31: @jiaplin1026_19, @brunch36l4_42, @knithink_60, @song-sawon_26, @bpmb_152, @ahala_117, @dldnfla0700_12, @tnlfl20_21\n",
      "Nearest to @alexkang_1012: @iamyoungzoo_17, @syn701_75, @columnist_186, @aerinitu_569, @citcoaching_44, @comento_103, @haksookim_112, @lookfar_27\n",
      "Nearest to @altctrlshift_35: @amberjeon48_425, @crispwatch_21, @jangsypd_3, @honeytip_874, @dryjshin_11, @sonst7_98, @yunj814_94, @yoosuusu_59\n",
      "Nearest to @13july_65: @failit_28, @puding_76, @nonamestudy_31, @timevoyage_60, @ansyd_477, @day-a_223, @windydog_5, @slist_1326\n",
      "Average loss at step 110000 : 6.448005513912356\n",
      "Average loss at step 120000 : 6.144003600481191\n",
      "Average loss at step 130000 : 5.864798366896342\n",
      "Average loss at step 140000 : 5.70767931736238\n",
      "Average loss at step 150000 : 5.267862066792079\n",
      "Average loss at step 160000 : 5.072270672356273\n",
      "Average loss at step 170000 : 5.125091818077998\n",
      "Average loss at step 180000 : 5.149523652439891\n",
      "Average loss at step 190000 : 4.924531142892555\n",
      "Average loss at step 200000 : 4.893850996077712\n",
      "Nearest to @artinsight_399: @dungdang_30, @windydog_12, @sunami1617_17, @isarerus_181, @slist_1326, @tnwlsl0810_16, @sting762_157, @jumjan_257\n",
      "Nearest to @bestar_31: @brunch36l4_42, @jiaplin1026_19, @knithink_60, @dldnfla0700_12, @bpmb_152, @onestepculture_203, @ahala_117, @song-sawon_26\n",
      "Nearest to @alexkang_1012: @iamyoungzoo_17, @comento_103, @haksookim_112, @copydory_80, @ljg523_1753, @aerinitu_569, @syn701_75, @lookfar_27\n",
      "Nearest to @altctrlshift_35: @crispwatch_21, @dryjshin_11, @honeytip_874, @amberjeon48_425, @jangsypd_3, @yunj814_94, @sonst7_98, @speralist_230\n",
      "Nearest to @13july_65: @failit_28, @heogoon_33, @nonamestudy_31, @puding_76, @mrlees_11, @windydog_5, @jjayjjay_1, @suffer-surfer_169\n",
      "Average loss at step 210000 : 4.7017806442938515\n",
      "Average loss at step 220000 : 5.013389110255649\n",
      "Average loss at step 230000 : 4.486437465406695\n",
      "Average loss at step 240000 : 4.656532268224564\n",
      "Average loss at step 250000 : 4.4035182511449396\n",
      "Average loss at step 260000 : 4.364547264971689\n",
      "Average loss at step 270000 : 4.236296714853111\n",
      "Average loss at step 280000 : 4.182356315284603\n",
      "Average loss at step 290000 : 4.188372856685031\n",
      "Average loss at step 300000 : 4.08000736698733\n",
      "Nearest to @artinsight_399: @sunami1617_17, @dungdang_30, @windydog_12, @jumjan_257, @peregrino97_372, @tnwlsl0810_16, @sting762_157, @zzangdae_33\n",
      "Nearest to @bestar_31: @brunch36l4_42, @dldnfla0700_12, @jiaplin1026_19, @knithink_60, @bpmb_152, @yettie_82, @ahala_117, @madamesnoopy_68\n",
      "Nearest to @alexkang_1012: @iamyoungzoo_17, @ljg523_1753, @haksookim_112, @copydory_80, @lookfar_27, @yoosuusu_45, @comento_103, @estherlee46ol_11\n",
      "Nearest to @altctrlshift_35: @dryjshin_11, @honeytip_874, @amberjeon48_425, @jangsypd_3, @crispwatch_21, @sonst7_98, @stonebrandcomm_3, @joogangl_103\n",
      "Nearest to @13july_65: @failit_28, @needleworm_10, @heogoon_33, @suffer-surfer_169, @nonamestudy_31, @windydog_5, @hotelscombined_52, @gians_13\n",
      "Average loss at step 310000 : 4.020723499723118\n",
      "Average loss at step 320000 : 4.011844353067525\n",
      "Average loss at step 330000 : 3.9374107495246164\n",
      "Average loss at step 340000 : 3.8953268395272143\n",
      "Average loss at step 350000 : 3.9251448648924008\n",
      "Average loss at step 360000 : 3.8331388023713253\n",
      "Average loss at step 370000 : 3.916929669413645\n",
      "Average loss at step 380000 : 3.9968941468856296\n",
      "Average loss at step 390000 : 3.7657810137471532\n",
      "Average loss at step 400000 : 3.7646461268586187\n",
      "Nearest to @artinsight_399: @sunami1617_17, @peregrino97_372, @minimalmind88_26, @jumjan_257, @sting762_157, @isarerus_181, @atelierjryu_2, @windydog_12\n",
      "Nearest to @bestar_31: @brunch36l4_42, @knithink_60, @bpmb_152, @dldnfla0700_12, @silvermouse_60, @yettie_82, @jiaplin1026_19, @ahala_117\n",
      "Nearest to @alexkang_1012: @iamyoungzoo_17, @ljg523_1753, @haksookim_112, @copydory_80, @lookfar_27, @garbar_622, @myungsunkim_269, @namoo_12\n",
      "Nearest to @altctrlshift_35: @dryjshin_11, @honeytip_874, @amberjeon48_425, @jangsypd_3, @crispwatch_21, @joogangl_103, @yunj814_94, @sonst7_98\n",
      "Nearest to @13july_65: @failit_28, @everj76_161, @suffer-surfer_169, @heogoon_33, @headbaker70_120, @chogabi_53, @koreakoala_385, @gians_13\n",
      "Average loss at step 410000 : 3.797335917126527\n",
      "Average loss at step 420000 : 3.875041539809188\n",
      "Average loss at step 430000 : 3.7240479218142775\n",
      "Average loss at step 440000 : 3.74544166330077\n",
      "Average loss at step 450000 : 3.766049434320326\n",
      "Average loss at step 460000 : 3.744795121545859\n",
      "Average loss at step 470000 : 3.6105133322980256\n",
      "Average loss at step 480000 : 3.6010270480482256\n",
      "Average loss at step 490000 : 3.5242434817298434\n",
      "Average loss at step 500000 : 3.5384738592315785\n",
      "Nearest to @artinsight_399: @jumjan_257, @sunami1617_17, @lisashin_11, @atelierjryu_2, @tnwlsl0810_16, @peregrino97_372, @sdain-sygang_58, @jaehyukjung_236\n",
      "Nearest to @bestar_31: @knithink_60, @brunch36l4_42, @silvermouse_60, @dldnfla0700_12, @sfac_504, @madamesnoopy_68, @yettie_82, @bpmb_152\n",
      "Nearest to @alexkang_1012: @iamyoungzoo_17, @copydory_80, @haksookim_112, @sunsutu_145, @lookfar_27, @aerinitu_569, @myungsunkim_269, @ljg523_1753\n",
      "Nearest to @altctrlshift_35: @honeytip_874, @jangsypd_3, @dryjshin_11, @crispwatch_21, @ujinnew_7, @yunj814_94, @pedaling_16, @amberjeon48_425\n",
      "Nearest to @13july_65: @failit_28, @suffer-surfer_169, @ryuinhwan_257, @headbaker70_120, @heogoon_33, @gians_13, @chogabi_53, @hotelscombined_52\n",
      "Average loss at step 510000 : 3.5904926717494496\n",
      "Average loss at step 520000 : 3.535577336625202\n",
      "Average loss at step 530000 : 3.6652584393028462\n",
      "Average loss at step 540000 : 3.3864005845497713\n",
      "Average loss at step 550000 : 3.545661715036128\n",
      "Average loss at step 560000 : 3.542325586886343\n",
      "Average loss at step 570000 : 3.3562045933922753\n",
      "Average loss at step 580000 : 3.4396207649620365\n",
      "Average loss at step 590000 : 3.558907467631511\n",
      "Average loss at step 600000 : 3.3884981610845775\n",
      "Nearest to @artinsight_399: @jaol_1, @jumjan_257, @atelierjryu_2, @sunami1617_17, @kyleso_7, @fullssamh7ty_827, @tnwlsl0810_16, @ddism2686_13\n",
      "Nearest to @bestar_31: @knithink_60, @brunch36l4_42, @silvermouse_60, @dldnfla0700_12, @sfac_504, @madamesnoopy_68, @yettie_82, @bpmb_152\n",
      "Nearest to @alexkang_1012: @iamyoungzoo_17, @haksookim_112, @lookfar_27, @copydory_80, @aerinitu_569, @myungsunkim_269, @2691999_311, @garbar_622\n",
      "Nearest to @altctrlshift_35: @honeytip_874, @jangsypd_3, @dryjshin_11, @crispwatch_21, @ujinnew_7, @nkt109_273, @pedaling_16, @yunj814_94\n",
      "Nearest to @13july_65: @everj76_161, @wushuwriter_20, @headbaker70_120, @ryuinhwan_257, @suffer-surfer_169, @maum-farm_874, @gians_13, @heogoon_33\n",
      "Average loss at step 610000 : 3.472364867875868\n",
      "Average loss at step 620000 : 3.360034726017178\n",
      "Average loss at step 630000 : 3.2638126738528954\n",
      "Average loss at step 640000 : 3.2965499097080566\n",
      "Average loss at step 650000 : 3.175583398812718\n",
      "Average loss at step 660000 : 3.358443523618439\n",
      "Average loss at step 670000 : 3.318956346188928\n",
      "Average loss at step 680000 : 3.221326079306612\n",
      "Average loss at step 690000 : 3.1953784170346453\n",
      "Average loss at step 700000 : 3.194740896746078\n",
      "Nearest to @artinsight_399: @jumjan_257, @jaol_1, @sterdam_962, @atelierjryu_2, @kyleso_7, @peregrino97_372, @tnwlsl0810_16, @sting762_97\n",
      "Nearest to @bestar_31: @knithink_60, @brunch36l4_42, @silvermouse_60, @sfac_504, @yettie_82, @madamesnoopy_68, @bpmb_152, @dldnfla0700_12\n",
      "Nearest to @alexkang_1012: @iamyoungzoo_17, @haksookim_112, @copydory_80, @lookfar_27, @boboc_93, @myungsunkim_269, @2691999_311, @aerinitu_569\n",
      "Nearest to @altctrlshift_35: @honeytip_874, @wikitree_3196, @dryjshin_11, @jangsypd_3, @ujinnew_7, @allstay_1044, @pedaling_16, @yunj814_94\n",
      "Nearest to @13july_65: @wushuwriter_20, @everj76_161, @ryuinhwan_257, @headbaker70_120, @maum-farm_874, @leegoeun_32, @gians_13, @brunch_145\n",
      "Average loss at step 710000 : 3.219439174212236\n",
      "Average loss at step 720000 : 3.177317620694853\n",
      "Average loss at step 730000 : 3.199398920587823\n",
      "Average loss at step 740000 : 3.4349588970135985\n",
      "Average loss at step 750000 : 3.1703092507405874\n",
      "Average loss at step 760000 : 3.1801645571694013\n",
      "Average loss at step 770000 : 3.1379765294520605\n",
      "Average loss at step 780000 : 3.1417951242400335\n",
      "Average loss at step 790000 : 3.2187727420620895\n",
      "Average loss at step 800000 : 3.153766424248018\n",
      "Nearest to @artinsight_399: @jumjan_257, @jaol_1, @project1_42, @kyleso_7, @atelierjryu_2, @sosoceo_146, @lucidjudge_128, @jaehyukjung_236\n",
      "Nearest to @bestar_31: @knithink_60, @silvermouse_60, @brunch36l4_42, @yettie_82, @madamesnoopy_68, @sfac_504, @likewater_23, @barneconomy_79\n",
      "Nearest to @alexkang_1012: @iamyoungzoo_17, @haksookim_112, @brunch_147, @boboc_93, @2691999_311, @heaven_76, @copydory_80, @myungsunkim_269\n",
      "Nearest to @altctrlshift_35: @wikitree_3196, @honeytip_874, @dryjshin_11, @ujinnew_7, @pedaling_16, @leedh200_10, @jangsypd_3, @warandpeace_27\n",
      "Nearest to @13july_65: @syshine7_36, @headbaker70_120, @everj76_161, @wushuwriter_20, @ryuinhwan_257, @maum-farm_874, @jade_366, @brunch_144\n",
      "Average loss at step 810000 : 3.1598999369146696\n",
      "Average loss at step 820000 : 3.3377964888111453\n",
      "Average loss at step 830000 : 3.135196910484981\n",
      "Average loss at step 840000 : 3.057928671418707\n",
      "Average loss at step 850000 : 3.1150177615317305\n",
      "Average loss at step 860000 : 3.059724055104731\n",
      "Average loss at step 870000 : 3.0315386469038814\n",
      "Average loss at step 880000 : 3.0437265814136394\n",
      "Average loss at step 890000 : 2.9987909339279337\n",
      "Average loss at step 900000 : 3.103880944727038\n",
      "Nearest to @artinsight_399: @sonujung_36, @ulfit_56, @jumjan_257, @project1_42, @jaol_1, @atelierjryu_2, @buddle_58, @hansuryeon_21\n",
      "Nearest to @bestar_31: @knithink_60, @bpmb_152, @seanchoi-hr_78, @silvermouse_60, @yettie_82, @madamesnoopy_68, @readingboy_43, @brunch36l4_42\n",
      "Nearest to @alexkang_1012: @iamyoungzoo_17, @heaven_76, @boboc_93, @indigoblue_69, @haksookim_112, @strategyhacker_241, @2691999_311, @dmlii_5\n",
      "Nearest to @altctrlshift_35: @honeytip_874, @wikitree_3196, @dryjshin_11, @ujinnew_7, @brunchqxk5_181, @warandpeace_27, @stonebrandcomm_3, @pedaling_16\n",
      "Nearest to @13july_65: @headbaker70_120, @wushuwriter_20, @everj76_161, @failit_28, @jade_366, @maum-farm_874, @ryuinhwan_257, @insateam_193\n",
      "Average loss at step 910000 : 2.9755002985371277\n",
      "Average loss at step 920000 : 2.9944033850909166\n",
      "Average loss at step 930000 : 3.0249399841866746\n",
      "Average loss at step 940000 : 2.924106940652531\n",
      "Average loss at step 950000 : 2.982040050971255\n",
      "Average loss at step 960000 : 3.046049286667441\n",
      "Average loss at step 970000 : 3.3518210748388433\n",
      "Average loss at step 980000 : 2.880587371995021\n",
      "Average loss at step 990000 : 2.8973002415534737\n",
      "Average loss at step 1000000 : 2.9489546631242147\n",
      "Nearest to @artinsight_399: @jaol_1, @ulfit_56, @atelierjryu_2, @leejion_36, @sonujung_36, @jumjan_257, @fullpompos_2, @project1_42\n",
      "Nearest to @bestar_31: @knithink_60, @yettie_82, @seanchoi-hr_78, @song-sawon_26, @silvermouse_60, @likewater_23, @readingboy_43, @comeintothe_147\n",
      "Nearest to @alexkang_1012: @iamyoungzoo_17, @boboc_93, @strategyhacker_241, @haksookim_112, @mobiinside_1177, @heaven_76, @cheeky_6, @indigoblue_69\n",
      "Nearest to @altctrlshift_35: @wikitree_3196, @honeytip_874, @dryjshin_11, @warandpeace_27, @yunj814_94, @brunchqxk5_181, @leedh200_10, @pedaling_16\n",
      "Nearest to @13july_65: @headbaker70_120, @everj76_161, @maum-farm_874, @failit_28, @jade_366, @wushuwriter_20, @ryuinhwan_257, @skykamja24_16\n",
      "1475.2742183208466 sec\n"
     ]
    }
   ],
   "source": [
    "# Step 5: skip-gram 모델 학습\n",
    "# 4시 4분 시작\n",
    "start_time = time.time()\n",
    "num_steps = 1000001\n",
    "data = corpus_r #################################### 수정\n",
    "ordered_words = list(word_to_id.keys())\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    average_loss, data_index = 0, 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels, data_index = generate_batch(data, batch_size, num_skips, skip_window, data_index)\n",
    "\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        # 마지막 10000번에 대한 평균 loss 표시\n",
    "        if step % 10000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 10000\n",
    "            print('Average loss at step {} : {}'.format(step, average_loss))\n",
    "            average_loss = 0\n",
    "        \n",
    "        # 10만번째마다 valid size만큼 sim 계산\n",
    "        if step % 100000 == 0:\n",
    "            sim = similarity.eval()         # (16, vocab_size)\n",
    "\n",
    "            for i in range(valid_size):\n",
    "                valid_word = ordered_words[valid_examples[i]]\n",
    "\n",
    "                top_k = 8\n",
    "                nearest = sim[i].argsort()[-top_k - 1:-1][::-1]\n",
    "                log_str = ', '.join([ordered_words[k] for k in nearest])\n",
    "                print('Nearest to {}: {}'.format(valid_word, log_str))\n",
    "\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "print(time.time() - start_time , 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(directory+'article_embedding_matrix_t',final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embeddings = np.load(directory+'article_embedding_matrix_t.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_matrix_multiplication(matrix, vector):\n",
    "    \"\"\"\n",
    "    Calculating pairwise cosine distance using matrix vector multiplication.\n",
    "    \"\"\"\n",
    "    dotted = matrix.dot(vector)\n",
    "    matrix_norms = np.linalg.norm(matrix, axis=1)\n",
    "    vector_norm = np.linalg.norm(vector)\n",
    "    matrix_vector_norms = np.multiply(matrix_norms, vector_norm)\n",
    "    neighbors = np.divide(dotted, matrix_vector_norms)\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04402565956115723 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "cos_matrix_multiplication(final_embeddings, final_embeddings[0])\n",
    "\n",
    "print (\"%s sec\"%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(directory + 'word_to_id.json') as f:\n",
    "    word_to_id  = json.load(f)\n",
    "\n",
    "with open(directory + 'id_to_word.json') as f:\n",
    "    id_to_word = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505842\n",
      "134994\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_id))\n",
    "print(len(id_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "W0721 17:39:03.793826 22088 utils_any2vec.py:354] duplicate word 'UNK' in D:/ANACONDA/envs/tf-gpu/code/NLP/kakao/data/gensim.txt, ignoring all but first\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.5535569190979 sec\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "st_t = time.time()\n",
    "tokenizer = Tokenizer()\n",
    "f = open(directory+'gensim.txt' ,'w', encoding='utf8')\n",
    "f.write(f'{len(id_to_word)} {128}\\n')\n",
    "\n",
    "word_vecs = final_embeddings  # W Matrix\n",
    "\n",
    "for idx, word in id_to_word.items():\n",
    "    str_vec = ' '.join(map(str, list(word_vecs[int(idx), :])))\n",
    "    f.write(f'{word} {str_vec}\\n')\n",
    "\n",
    "f.close()\n",
    "w2v_cbow = KeyedVectors.load_word2vec_format(directory+'gensim.txt', unicode_errors='ignore')\n",
    "print(time.time() - st_t,'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11109757423400879\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "w2v_cbow.most_similar(id_to_word['0'])\n",
    "print(time.time() -s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = w2v_cbow.most_similar(id_to_word['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 134994/134994 [10:36<00:00, 212.18it/s]\n"
     ]
    }
   ],
   "source": [
    "wl = []\n",
    "for i in tqdm(range(len(id_to_word)),mininterval = 1):\n",
    "    li = []\n",
    "    li = w2v_cbow.most_similar(id_to_word[str(i)])\n",
    "    li_1 = []\n",
    "    for j in range(10):\n",
    "        li_1.append(li[j][0])\n",
    "    wl.append(li_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134994, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wl =  np.asarray(wl)\n",
    "print(wl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(directory+'article_similarity_t',wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_similarity = np.load(directory+'article_similarity_t.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@yuhaaustin_52', '@yuhaaustin_54', '@yuhaaustin_10',\n",
       "       '@spring0915_57', '@hukho_235', '@leesoltoon_33', '@deckey1985_51',\n",
       "       '@anetmom_8', '@ohmygod_39', '@sunnysohn_60'], dtype='<U21')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_similarity[word_to_id['@yuhaaustin_45']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
